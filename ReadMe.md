# ğŸ’¬ RAG Local avec LM Studio

Ce projet met en place un **RAG (Retrieval-Augmented Generation)** 100% local sous Windows et macOS.

- [LM Studio](https://lmstudio.ai) â†’ gÃ©nÃ©ration de texte (LLM local, API OpenAI-compatible)
- [ChromaDB](https://www.trychroma.com/) â†’ indexation vectorielle
- [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) â†’ modÃ¨le dâ€™embeddings multilingue (FR inclus)
- [Streamlit](https://streamlit.io/) â†’ interface web type ChatGPT

ğŸ‘‰ **Objectif** : poser des questions sur vos propres documents (.pdf, .docx, .txt, .xlsx) et obtenir des rÃ©ponses sourcÃ©es.

Exemple d'utilisation : 
![Exemple dâ€™utilisation](Pictures/image.png)


---

## ğŸ¤” Quâ€™est-ce quâ€™un RAG ?

Un **RAG (Retrieval-Augmented Generation)** combine deux Ã©tapes :

1. **Retrieval** : recherche des passages pertinents dans vos documents, grÃ¢ce Ã  des embeddings stockÃ©s dans une base vectorielle.
2. **Augmented Generation** : le LLM gÃ©nÃ¨re une rÃ©ponse en utilisant ces passages comme contexte.

â¡ï¸ RÃ©sultat : des rÃ©ponses **fiables, contextualisÃ©es et locales**.

---

## âš™ï¸ Arborescence du projet
```bash
rag_lmstudio/
â”‚â”€â”€ docs/ # Vos documents (peut contenir des sous-dossiers)
â”‚â”€â”€ db/ # Base vectorielle locale (Chroma)
â”‚â”€â”€ .env # Variables d'environnement (API LM Studio, modÃ¨le)
â”‚â”€â”€ requirements.txt # DÃ©pendances Python
â”‚â”€â”€ utils_loaders.py # Fonctions de lecture des fichiers
â”‚â”€â”€ build_index.py # Script dâ€™indexation incrÃ©mentale
â”‚â”€â”€ app.py # Interface web Streamlit (chat)
â”‚â”€â”€ README.md # Documentation du projet
```
---


---

## ğŸš€ Installation

### 1. PrÃ©-requis
- **Python 3.10+**
- [LM Studio](https://lmstudio.ai) installÃ©
- **NVIDIA GPU** (CUDA) *ou* **Mac (MPS/CPU)** *ou* CPU seul

### 2. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```


(choisissez dans le fichier la bonne installation de PyTorch selon Windows/macOS)


### 3. Configurer LM Studio
- Ouvrir LM Studio
- Aller dans Developer â†’ Local Server et cliquer sur Start
- Charger un modÃ¨le (ex. llama-3.1-8b-instruct)
- VÃ©rifier quâ€™il apparaÃ®t dans la liste des modÃ¨les servis
- âš ï¸ DÃ©cochez Allow local network access pour rester en localhost.



ğŸ”‘ Configuration .env

CrÃ©ez un fichier .env Ã  la racine :

```Bash
OPENAI_BASE_URL=http://127.0.0.1:1234/v1
OPENAI_API_KEY=lm-studio
LMSTUDIO_MODEL=llama-3.1-8b-instruct   # nom exact du modÃ¨le dans LM Studio
```

ğŸ“¥ Indexation des documents

DÃ©posez vos fichiers dans docs/ (les sous-dossiers sont pris en charge).
Lancez ensuite :

```Bash
python build_index.py
```
- Les documents sont dÃ©coupÃ©s en chunks
- Les embeddings sont calculÃ©s (GPU si dispo â†’ sinon MPS/CPU)
- La base vectorielle est sauvegardÃ©e dans db/

ğŸ’¬ Lancer lâ€™interface
```Bash
streamlit run app.py
```

Une page web devrait s'ouvrir sinon : http://localhost:8501





FonctionnalitÃ©s :

- Interface style ChatGPT
- RÃ©ponses affichÃ©es en streaming
- Bouton ğŸ” Reconstruire lâ€™index
- Affichage des sources et passages utilisÃ©s


## Auteur

Arthur Prigent  
